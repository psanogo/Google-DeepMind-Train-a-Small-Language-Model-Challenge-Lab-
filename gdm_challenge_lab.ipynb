{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DAsej7XSlYt"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations GitHub README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3myzQnLMOJ91"
      },
      "source": [
        "<img src=\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C1-white-bg.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE0jaJsaICiX"
      },
      "source": [
        "# Build Your Own Small Language Model: Challenge Lab\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yoo6I-7_56N"
      },
      "source": [
        "## Challenge Scenario\n",
        "\n",
        "### Cymbal Chat: Developing a chatbot for the Arabic speaking market.\n",
        "\n",
        "Cymbal Chat is an AI language modeling small startup. They would like to expand the languages that they cover to also include Arabic. It has a very different grammar than English, and uses a different character set than what their AI researchers are used to. You are going to help them with their first steps: exploring the use of character-based language models.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info**\n",
        ">\n",
        "> Sometimes character-based language models may work better for Arabic NLP tasks than word-based models [1]. It is because subwords can be attached to the beginning or end of words to add meaning. For example, a noun (book) can be modified as:\n",
        ">\n",
        ">* **ÙƒØªØ§Ø¨ (kitÄb)** - a book\n",
        ">* **ÙƒØªØ§Ø¨ÙŠ (kitÄb-Ä«)** - my book\n",
        ">* **ÙƒØªØ§Ø¨Ùƒ (kitÄb-uk)** - your (masc.) book\n",
        ">* **ÙƒØªØ§Ø¨Ù‡ (kitÄb-uh)** - his book\n",
        ">\n",
        ">Arabic is read from *right to left*, and here the modifiers were added to the end of the host word (book).\n",
        ">They can also be added before the host word, for example conjunctions like \"Ùˆ \" (wa-, meaning 'and') and \"Ù \" (fa-, meaning 'so' or 'then'), as well as prepositions such as \"Ø¨ \" (bi-, meaning 'with' or 'in') and \"Ù„ \" (li-, meaning 'for' or 'to').\n",
        "------\n",
        "\n",
        "<br />\n",
        "\n",
        "Exploring the use of character-based language models will involve these Tasks:\n",
        "\n",
        "* **Task 1.** *Define helper functions and load data.*\n",
        "* **Task 2.** *Character tokenizer*: Create a simple character level tokenizer for Arabic text.\n",
        "* **Task 3.** *N-gram text generator*: Develop an n-gram based text generator as a baseline against which to evaluate a more sophisticated model.\n",
        "* **Task 4.** *Data preparation*: Prepare an Arabic dataset so that it can be used for training a character-based transformer language model on restricted resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwwgTmVkFvte"
      },
      "source": [
        "## Task 1.  Define helper functions and load data\n",
        "\n",
        "You will use a small dataset of short children's stories in Arabic to explore the use of character-based language models for Cymbal Chat.\n",
        "\n",
        "For this task, you will import the libraries required for this lab and load and display the dataset of Arabic stories.\n",
        "\n",
        "âœ… All code cells have already been written for you in all parts of this task. **You are not required to add or modify code for Task 1.**\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> ğŸ’» **Your task**:\n",
        ">\n",
        "> Run the cells in **Task 1** to load a small children's stories dataset.\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fbVdcGBBZaLu"
      },
      "outputs": [],
      "source": [
        "# Install logging packages for lab grading\n",
        "%pip install --upgrade google-cloud-logging > /dev/null 2>&1\n",
        "%pip install --force-reinstall protobuf==3.20.0 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GH-Z1t7s_56N"
      },
      "outputs": [],
      "source": [
        "# Packages used.\n",
        "from collections import Counter, defaultdict\n",
        "import random\n",
        "import re\n",
        "from typing import Any, Literal\n",
        "import itertools\n",
        "\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.cloud import logging as gcp_logging\n",
        "from google.cloud.logging.handlers import CloudLoggingHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_KL6PBLQZaLw"
      },
      "outputs": [],
      "source": [
        "# Instantiate the Google Cloud Logging client\n",
        "import logging\n",
        "from google.cloud import logging as gcp_logging\n",
        "from google.cloud.logging.handlers import CloudLoggingHandler\n",
        "\n",
        "# Instantiate the Google Cloud Logging client\n",
        "client = gcp_logging.Client()\n",
        "\n",
        "# Create a specific logger for cloud logging\n",
        "logger = logging.getLogger('my_cloud_logger')\n",
        "\n",
        "# Prevent this logger from sending messages to its parent (the root logger),\n",
        "# which has the default console handler\n",
        "logger.propagate = False\n",
        "\n",
        "# Create and add the Cloud Logging handler\n",
        "handler = CloudLoggingHandler(client)\n",
        "logger.addHandler(handler)\n",
        "\n",
        "# Set the logging level\n",
        "logger.setLevel(logging.INFO)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzvpmQyv_56O"
      },
      "source": [
        "### Helper functions\n",
        "\n",
        "------\n",
        "> **â„¹ï¸ Info: Working with Arabic text**\n",
        ">\n",
        ">An Arabic sentence like\n",
        ">\n",
        ">Ø°ÙÙ‡ÙØ¨Ù Ø§Ù„Ø·ÙØ§Ù„Ø¨Ù Ø§Ù„Ù‰ Ø§Ù„Ù…ÙØ¯Ù’Ø±ÙØ³ÙØ©\n",
        ">\n",
        ">(meaning *\"the student went to school\"*) is read from *right to left*. The first character in the string is printed on the far right where one starts reading, and programatically still has string index `0`. The string can contain:\n",
        ">\n",
        ">* **Bidirectional control characters** that states that the text should be displayed right-to-left on a screen. (These additional control characters do not change Python indexing.)\n",
        ">\n",
        ">* **Diacritic characters** or markings that are placed above or below letters to indicate, for instance, short vowel sounds. They can help to disambiguate text.\n",
        "-----\n",
        "\n",
        "<br />\n",
        "\n",
        "The helper functions `clean_bidi_chars` and `remove_diacritics` will be used to preprocess text by removing bidirectional control characters, as well as diacritic markings that are not essential for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H_lA75qu_56O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed2dca2-9e05-47c8-e658-261fe5e86088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EXAMPLE\n",
            "Text:           Ø°ÙÙ‡ÙØ¨Ù Ø§Ù„Ø·ÙØ§Ù„Ø¨Ù Ø§Ù„Ù‰ Ø§Ù„Ù…ÙØ¯Ù’Ø±ÙØ³ÙØ©\n",
            "Processed text: â€®Ø°Ù‡Ø¨ Ø§Ù„Ø·Ø§Ù„Ø¨ Ø§Ù„Ù‰ Ø§Ù„Ù…Ø¯Ø±Ø³Ø©â€¬\n"
          ]
        }
      ],
      "source": [
        "def clean_bidi_chars(text: str) -> str:\n",
        "   \"\"\"Remove Unicode bidirectional control characters from text.\n",
        "\n",
        "   Args:\n",
        "        text: Input text that may contain BiDi control characters.\n",
        "\n",
        "   Returns:\n",
        "       Text with all bidirectional control characters removed.\n",
        "   \"\"\"\n",
        "   bidi_chars = re.compile(r\"[\\u202A-\\u202E\\u2066-\\u2069]\")\n",
        "   return bidi_chars.sub(\"\", text)\n",
        "\n",
        "\n",
        "def remove_diacritics(text: str) -> str:\n",
        "   \"\"\"Remove Arabic diacritic characters to simplify text.\n",
        "\n",
        "   Args:\n",
        "       text: Arabic text that may contain diacritical marks.\n",
        "\n",
        "   Returns:\n",
        "       Text with all Arabic diacritics and tatweel removed.\n",
        "   \"\"\"\n",
        "   arabic_diacritics = re.compile(r\"[\\u064B-\\u065F\\u0670\\u0640]\")\n",
        "   return arabic_diacritics.sub(\"\", text)\n",
        "\n",
        "\n",
        "def display_arabic(text: str) -> str:\n",
        "   \"\"\"Format Arabic text terminals with limited right-to-left support.\n",
        "\n",
        "   Args:\n",
        "       text: Arabic text to format for display.\n",
        "\n",
        "   Returns:\n",
        "       Text wrapped with RTL (right-to-left) override characters for improved\n",
        "       terminal rendering.\n",
        "   \"\"\"\n",
        "   return \"\\u202E\" + text + \"\\u202C\"\n",
        "\n",
        "\n",
        "sentence = \"Ø°ÙÙ‡ÙØ¨Ù Ø§Ù„Ø·ÙØ§Ù„Ø¨Ù Ø§Ù„Ù‰ Ø§Ù„Ù…ÙØ¯Ù’Ø±ÙØ³ÙØ©\"  # The student went to school\n",
        "sentence_bidi_clean = clean_bidi_chars(sentence)\n",
        "sentence_without_diacritics = remove_diacritics(sentence_bidi_clean)\n",
        "\n",
        "print(\"EXAMPLE\")\n",
        "print(f\"Text:           {sentence}\")\n",
        "print(f\"Processed text: {display_arabic(sentence_without_diacritics)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baiE_bsa_56O"
      },
      "source": [
        "### Load and display the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "z4pPspyK_56P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fd4cd01-af7f-48bc-aa2a-487540a532a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First story after preprocessing:\n",
            "\tâ€®Ø§Ù„Ø´Ù…Ø³ Ø·Ù„Ø¹Øª! Ø¨Ø·Ø© ØµØºÙŠØ±Ø© ØªØ³ØªÙŠÙ‚Ø¸. ØªØ­Ø¨ Ø§Ù„Ù„Ø¹Ø¨. ØªØ±Ù‰ Ù…Ø²Ø§Ø±Ø¹Ø§ Ù‚Ø±Ø¨ Ø¨Ø±ÙƒØªÙ‡Ø§. Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ Ù„Ø·ÙŠÙ. ÙŠÙ„ÙˆØ­ Ù„Ù„Ø¨Ø·Ø©.\n",
            "\n",
            "Ø§Ù„Ø¨Ø·Ø© ØªØªÙ‡Ø§Ø¯Ù‰ Ø¥Ù„Ù‰ ØµØ¯ÙŠÙ‚Ù‡Ø§ØŒ Ø£Ø±Ù†Ø¨ Ù†Ø§Ø¹Ù…. Ø§Ù„Ø£Ø±Ù†Ø¨ ÙŠØ£ÙƒÙ„ Ø¬Ø²Ø±Ø©. ØªÙ‚ÙˆÙ„ Ø§Ù„Ø¨Ø·Ø©: \"Ù…Ø±Ø­Ø¨Ø§! Ù‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù„Ø¹Ø¨ØŸ\" Ø§Ù„Ø£Ø±Ù†Ø¨ ÙŠÙˆÙ…Ø¦ Ø¨Ø±Ø£Ø³Ù‡. Ù‡Ù…Ø§ Ø³Ø¹ÙŠØ¯Ø§Ù†.\n",
            "\n",
            "ÙŠØ±ÙŠØ§Ù† ØµØ®Ø±Ø© Ù…Ù„Ø³Ø§Ø¡ ÙˆÙ…Ø³Ø·Ø­Ø©. ØªØ¨Ø¯Ùˆ Ù…ÙƒØ§Ù†Ø§ Ù…Ù…ØªØ¹Ø§ Ù„Ù„ØªØ²Ù„Ø¬! ÙŠØ­Ø§ÙˆÙ„ Ø§Ù„Ø£Ø±Ù†Ø¨ Ø§Ù„ØªØ²Ù„Ø¬ Ù„ÙƒÙ†Ù‡ ÙŠÙ‚ÙØ² Ø¨Ø¯Ù„Ø§ Ù…Ù† Ø°Ù„Ùƒ. Ø§Ù„Ø¨Ø·Ø© ØªÙ†Ø²Ù„Ù‚ Ù‚Ù„ÙŠÙ„Ø§. Ù…Ù† Ø§Ù„Ù…Ù…ØªØ¹ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©!\n",
            "\n",
            "Ù‚Ø±ÙŠØ¨Ø§ØŒ Ø­Ø§Ù† ÙˆÙ‚Øª Ø§Ù„ØºØ¯Ø§Ø¡. Ø§Ù„Ø¨Ø·Ø© ØªØ£ÙƒÙ„ Ø¨Ø°ÙˆØ±Ø§ Ù„Ø°ÙŠØ°Ø©. Ø§Ù„Ø£Ø±Ù†Ø¨ ÙŠØ£ÙƒÙ„ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø¬Ø²Ø±. ÙŠØ£Ø®Ø°Ø§Ù† Ù‚ÙŠÙ„ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø´Ù…Ø³ Ø§Ù„Ø¯Ø§ÙØ¦Ø©. Ù‡Ù…Ø§ ØµØ¯ÙŠÙ‚Ø§Ù† Ø­Ù…ÙŠÙ…Ø§Ù†.â€¬\n"
          ]
        }
      ],
      "source": [
        "# Load dataset of Arabic stories.\n",
        "url = \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/200-stories-ar.json\"\n",
        "df = pd.read_json(url)\n",
        "\n",
        "# Extract dataset from dataframe and preprocess text.\n",
        "dataset = []\n",
        "for story in df[\"story_ar\"].to_list():\n",
        "    story = clean_bidi_chars(story)\n",
        "    story = remove_diacritics(story)\n",
        "    dataset.append(story)\n",
        "\n",
        "first_story = dataset[0]\n",
        "print(f\"First story after preprocessing:\\n\\t{display_arabic(first_story)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABO_-UHj_56P"
      },
      "source": [
        "## Task 2. Character Tokenizer\n",
        "\n",
        "You need to convert natural language text to character-tokens and back again. For example, the text\n",
        "\n",
        "`text = \"Ø§Ù„Ø´Ù…Ø³ Ø·Ù„Ø¹Øª\"`\n",
        "\n",
        "(meaning: *\"The sun is out\"*) should be split into a list of character tokens.  The first three character tokens are:\n",
        "\n",
        "```\n",
        "tokens[0] == \"Ø§\"\n",
        "tokens[1] == \"Ù„\"\n",
        "tokens[2] == \"Ø´\"\n",
        "```\n",
        "\n",
        "Remember that the text literaral is formatted to appear on screen *right-to-left*. When the tokens are joined, the original text should be recreated.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxSEptLJ_56P"
      },
      "source": [
        "### Complete a simple Arabic character tokenizer.\n",
        "\n",
        "In this section, you will complete two methods in the `SimpleArabicCharacterTokenizer` class to\n",
        "convert natural language text to character-tokens and back.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> ğŸ’» **Your task**:\n",
        ">\n",
        "> Complete the **`character_tokenize`** method.\n",
        ">\n",
        "> Given text input like `text = \"Ø§Ù„Ø´Ù…Ø³ Ø·Ù„Ø¹Øª\"` (meaning: *\"The sun is out\"*), the method should return a list of character level tokens, `tokens`, each a `str` object of length one. These must be the characters of the input text in the same order. For the example text, it means `tokens[0]` is `\"Ø§\"`.\n",
        ">\n",
        "> *Note that if you print `tokens` to standard output it may also appear in right-to-left order. Like this:*\n",
        "> ```\n",
        "> [\"Ø§\", \"Ù„\", \"Ø´\", \"Ù…\", \"Ø³\", \" \", \"Ø·\", \"Ù„\", \"Ø¹\", \"Øª\"]\n",
        "> ```\n",
        "> *This is a consequence of the right-to-left encoding of the Arabic characters but it does not affect the reference order of the elements.*\n",
        "-----\n",
        "> ğŸ’» **Your task**:\n",
        ">\n",
        "> Complete the **`join_text`** method.\n",
        ">\n",
        "> Given a list of tokens, this should return a string containing the characters appearing in the input tokens.\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HG3Fu9vo_56P"
      },
      "outputs": [],
      "source": [
        "class SimpleArabicCharacterTokenizer:\n",
        "    \"\"\"A simple character tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def character_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given Arabic text into character tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split into character tokens.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "        # Tokenize the string into character tokens by converting it to a list.\n",
        "        tokens = list(text)\n",
        "        return tokens\n",
        "\n",
        "    def join_text(self, tokens: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string.\n",
        "\n",
        "        The combined tokens are combined simply into a single string.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined together without intervening characters.\n",
        "        \"\"\"\n",
        "        # Join the list of character tokens into a string.\n",
        "        text = \"\".join(tokens)\n",
        "        return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XFfs6AcQI0R"
      },
      "source": [
        "### Test your tokenizer\n",
        "\n",
        "You can experiment with your implementation of `SimpleArabicCharacterTokenizer` by running the cell below.\n",
        "\n",
        "âœ… All the code has been written for you in this part. **You are not required to add or modify code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "b-oH7-drQfRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bdf47bc-3c24-46c1-f40c-372744a91f2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line of dataset:\n",
            "\tØ§Ù„Ø´Ù…Ø³ Ø·Ù„Ø¹Øª! Ø¨Ø·Ø© ØµØºÙŠØ±Ø© ØªØ³ØªÙŠÙ‚Ø¸. ØªØ­Ø¨ Ø§Ù„Ù„Ø¹Ø¨. ØªØ±Ù‰ Ù…Ø²Ø§Ø±Ø¹Ø§ Ù‚Ø±Ø¨ Ø¨Ø±ÙƒØªÙ‡Ø§. Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ Ù„Ø·ÙŠÙ. ÙŠÙ„ÙˆØ­ Ù„Ù„Ø¨Ø·Ø©.\n",
            "First line tokens:\n",
            "\t['Ø§', 'Ù„', 'Ø´', 'Ù…', 'Ø³', ' ', 'Ø·', 'Ù„', 'Ø¹', 'Øª', '!', ' ', 'Ø¨', 'Ø·', 'Ø©', ' ', 'Øµ', 'Øº', 'ÙŠ', 'Ø±', 'Ø©', ' ', 'Øª', 'Ø³', 'Øª', 'ÙŠ', 'Ù‚', 'Ø¸', '.', ' ', 'Øª', 'Ø­', 'Ø¨', ' ', 'Ø§', 'Ù„', 'Ù„', 'Ø¹', 'Ø¨', '.', ' ', 'Øª', 'Ø±', 'Ù‰', ' ', 'Ù…', 'Ø²', 'Ø§', 'Ø±', 'Ø¹', 'Ø§', ' ', 'Ù‚', 'Ø±', 'Ø¨', ' ', 'Ø¨', 'Ø±', 'Ùƒ', 'Øª', 'Ù‡', 'Ø§', '.', ' ', 'Ø§', 'Ù„', 'Ù…', 'Ø²', 'Ø§', 'Ø±', 'Ø¹', ' ', 'Ù„', 'Ø·', 'ÙŠ', 'Ù', '.', ' ', 'ÙŠ', 'Ù„', 'Ùˆ', 'Ø­', ' ', 'Ù„', 'Ù„', 'Ø¨', 'Ø·', 'Ø©', '.']\n",
            "First line rejoined:\n",
            "\tØ§Ù„Ø´Ù…Ø³ Ø·Ù„Ø¹Øª! Ø¨Ø·Ø© ØµØºÙŠØ±Ø© ØªØ³ØªÙŠÙ‚Ø¸. ØªØ­Ø¨ Ø§Ù„Ù„Ø¹Ø¨. ØªØ±Ù‰ Ù…Ø²Ø§Ø±Ø¹Ø§ Ù‚Ø±Ø¨ Ø¨Ø±ÙƒØªÙ‡Ø§. Ø§Ù„Ù…Ø²Ø§Ø±Ø¹ Ù„Ø·ÙŠÙ. ÙŠÙ„ÙˆØ­ Ù„Ù„Ø¨Ø·Ø©.\n"
          ]
        }
      ],
      "source": [
        "# Test your tokenizer on the first line of the first story from the dataset.\n",
        "\n",
        "# Create tokenizer.\n",
        "tokenizer = SimpleArabicCharacterTokenizer()\n",
        "\n",
        "# Get first line of first story.\n",
        "first_story = dataset[0]\n",
        "first_line = first_story.split(\"\\n\")[0]\n",
        "print(f\"First line of dataset:\\n\\t{first_line}\")\n",
        "\n",
        "# Tokenize the line of text.\n",
        "first_line_tokens = tokenizer.character_tokenize(first_line)\n",
        "print(f\"First line tokens:\\n\\t{first_line_tokens}\")\n",
        "\n",
        "# Join the tokens to reform the line of text.\n",
        "first_line_rejoined =  tokenizer.join_text(first_line_tokens)\n",
        "print(f\"First line rejoined:\\n\\t{first_line_rejoined}\")\n",
        "\n",
        "# Do not remove or modify this logging call, it will be used for tracking purposes\n",
        "logger.info(f'Task 2: First line rejoined: {len(first_line_tokens)}')"
      ]
    },
    {
      "source": [
        "class SimpleArabicCharacterTokenizer:\n",
        "    \"\"\"A simple character tokenizer for Arabic text.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initializes the tokenizer.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def character_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given Arabic text into character tokens.\n",
        "\n",
        "        Args:\n",
        "            text: The input string to tokenize.\n",
        "\n",
        "        Returns:\n",
        "            A list of single-character strings.\n",
        "        \"\"\"\n",
        "        return list(text)\n",
        "\n",
        "    def join_text(self, tokens: list[str]) -> str:\n",
        "        \"\"\"Joins a list of character tokens back into a single string.\n",
        "\n",
        "        Args:\n",
        "            tokens: A list of single-character strings.\n",
        "\n",
        "        Returns:\n",
        "            The reconstructed string.\n",
        "        \"\"\"\n",
        "        return \"\".join(tokens)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HvLiLWw7bfqt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "source": [
        "class SimpleArabicCharacterTokenizer:\n",
        "    \"\"\"A simple character tokenizer.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def character_tokenize(self, text: str) -> list[str]:\n",
        "        \"\"\"Splits a given Arabic text into character tokens.\n",
        "\n",
        "        Args:\n",
        "            text: Text to split into character tokens.\n",
        "\n",
        "        Returns:\n",
        "            List of tokens after splitting `text`.\n",
        "        \"\"\"\n",
        "\n",
        "        # Tokenize the string into character tokens.\n",
        "        tokens = list(text)\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def join_text(self, tokens: list[str]) -> str:\n",
        "        \"\"\"Combines a list of tokens into a single string.\n",
        "\n",
        "        The combined tokens are combined simply into a single string.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of tokens to be joined.\n",
        "\n",
        "        Returns:\n",
        "            String with all tokens joined together without intervening characters.\n",
        "        \"\"\"\n",
        "\n",
        "        # Join the list of character tokens into a string.\n",
        "        text = \"\".join(tokens)\n",
        "\n",
        "        return text\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7aGlhXibbS1V"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RyiVsR0-ZaL0"
      },
      "source": [
        "Go to the **Task 2. Character Tokenizer** section of the lab instructions and click **Check my progress** to verify the objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CeK3Z8iS_56P"
      },
      "source": [
        "## Task 3. Generating text from an n-gram model\n",
        "\n",
        "Cymbal Chat wishes to create simple baseline for text generation, using an n-gram model. They already have code to build an n-gram model, which they adapted from *Lab: Experiment with N-Gram Models* from course *01 Build Your Own Small Language Model*.\n",
        "\n",
        "For Task 3, you will write code to generate text from their n-gram model, given a prompt. Your instructions will appear in the \"ğŸ’» **Your task**\" box after their n-gram model functions. First, run Cymbal Chat's functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c238JUep_56P"
      },
      "source": [
        "### N-gram model functions\n",
        "\n",
        "Cymbal Chat already have code to build an n-gram model. Their function `build_ngram_model` takes a corpus of text documents and a simple Arabic character tokenizer and builds an n-gram model from the data.\n",
        "\n",
        "Run the cell with Cymbal Chat's functions that build an n-gram model. **You are not required to modify any of these functions.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_xRrortP_56Q"
      },
      "outputs": [],
      "source": [
        "def generate_character_ngrams(\n",
        "        text: str,\n",
        "        n: int,\n",
        "        tokenizer: SimpleArabicCharacterTokenizer\n",
        ") -> list[tuple[str]]:\n",
        "    \"\"\"Generates character n-grams from a given text.\n",
        "\n",
        "    Args:\n",
        "        text: The input text string.\n",
        "        n: The size of the n-grams.\n",
        "        tokenizer: A tokenizer that converts text into character tokens.\n",
        "\n",
        "    Returns:\n",
        "        A list of n-grams, each represented as a list of character tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize text.\n",
        "    tokens = tokenizer.character_tokenize(text)\n",
        "\n",
        "    # Construct the list of n-grams.\n",
        "    ngrams = []\n",
        "\n",
        "    num_of_tokens = len(tokens)\n",
        "\n",
        "    # The last n-gram will be tokens[num_of_tokens - n + 1: num_of_tokens + 1].\n",
        "    for i in range(0, num_of_tokens - n + 1):\n",
        "        ngrams.append(tuple(tokens[i:i+n]))\n",
        "\n",
        "    return ngrams\n",
        "\n",
        "def get_character_ngram_counts(\n",
        "        dataset: list[str],\n",
        "        n: int,\n",
        "        tokenizer: SimpleArabicCharacterTokenizer\n",
        ") -> dict[str, Counter]:\n",
        "    \"\"\"Computes the character n-gram counts from a dataset.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, constructs n-grams from each text, and creates a dictionary where:\n",
        "\n",
        "    * Tokens are individual characters.\n",
        "    * Keys represent n-1 token long contexts `context`.\n",
        "    * Values are a Counter object `counts` such that `counts[next_token]` is the\n",
        "    * count of `next_token` following `context`.\n",
        "\n",
        "    Args:\n",
        "        dataset: The list of text strings in the dataset.\n",
        "        n: The size of the n-grams to generate (e.g., 2 for bigrams, 3 for\n",
        "            trigrams).\n",
        "        tokenizer: A tokenizer that converts text into character tokens.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary where keys are (n-1)-token contexts and values are Counter\n",
        "        objects storing the counts of each next token for that context.\n",
        "    \"\"\"\n",
        "    ngram_counts = defaultdict(Counter)\n",
        "\n",
        "    # Loop through all paragraphs.\n",
        "    for paragraph in dataset:\n",
        "        # Loop through all n-grams for the paragraph.\n",
        "        for ngram in generate_character_ngrams(paragraph, n, tokenizer):\n",
        "            # Extract the context. This will be all but the last token.\n",
        "            context = \"\".join(ngram[:-1])\n",
        "            # Extract the next token. This will be the last token of the n-gram.\n",
        "            next_token = ngram[-1]\n",
        "            # Increment the counter for the context - next_token pair by 1.\n",
        "            ngram_counts[context][next_token] += 1\n",
        "\n",
        "    return dict(ngram_counts)\n",
        "\n",
        "def build_ngram_model(\n",
        "        dataset: list[str],\n",
        "        n: int,\n",
        "        tokenizer: SimpleArabicCharacterTokenizer\n",
        ") -> dict[str, dict[str, float]]:\n",
        "    \"\"\"Builds an n-gram language model.\n",
        "\n",
        "    This function takes a list of text strings (paragraphs or sentences) as\n",
        "    input, generates n-grams from each text using the function get_ngram_counts\n",
        "    and converts them into probabilities. The resulting model is a dictionary,\n",
        "    where keys are (n-1)-token contexts and values are dictionaries mapping\n",
        "    possible next tokens to their conditional probabilities given the context.\n",
        "    When a conditional probability is requested for an unseen context the\n",
        "    model returns the marginal probability of that token.\n",
        "\n",
        "    Args:\n",
        "        dataset: A list of text strings representing the dataset.\n",
        "        n: The size of the n-grams (e.g., 2 for a bigram model).\n",
        "        tokenizer: A tokenizer that converts text into character tokens.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary representing the n-gram language model with fallback, where\n",
        "        keys are (n-1)-tokens contexts and values are dictionaries mapping\n",
        "        possible next tokens to their conditional probabilities.\n",
        "    \"\"\"\n",
        "    # A dictionary to store P(B | A).\n",
        "    ngram_model = {}\n",
        "\n",
        "    # Use the ngram_counts as computed by the get_ngram_counts function.\n",
        "    ngram_counts = get_character_ngram_counts(dataset, n, tokenizer)\n",
        "\n",
        "    # Compute Count(A) and P(B | A ) for observed contexts A.\n",
        "    for context, next_tokens in ngram_counts.items():\n",
        "        context_total_count = sum(next_tokens.values())\n",
        "        ngram_model[context] = {}\n",
        "        for token, count in next_tokens.items():\n",
        "            ngram_model[context][token] = count / context_total_count\n",
        "\n",
        "    return ngram_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW4TUi9V_56Q"
      },
      "source": [
        "### Complete a function that generates text from an n-gram model\n",
        "\n",
        "------\n",
        "> ğŸ’» **Your task:**\n",
        ">\n",
        "> Complete the `generate_text_from_ngram_model` function.\n",
        ">\n",
        "> You should add code to the `generate_text_from_ngram_model` function so that text can be generated using an n-gram model, `ngram_model`. Given a prompt like `start_prompt = \"ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯\"`, an additional `n_tokens` should be generated, either:\n",
        ">\n",
        "> * by randomly sampling the next token (`sampling_mode = \"random\"`), or\n",
        "> * by greedily picking the token with highest probability (`sampling_mode = \"greedy\"`)\n",
        ">\n",
        "> at each step.\n",
        ">\n",
        "> Further instructions are marked in the `generate_text_from_ngram_model` function that you should complete. The `argmax` function is useful for greedy sampling.\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GecaH3M2_56Q"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import Literal\n",
        "\n",
        "# This helper function is provided in the lab.\n",
        "def argmax(arr: list[float]) -> int:\n",
        "    \"\"\"Get the index of the largest element in list of float elements.\"\"\"\n",
        "    return max(range(len(arr)), key=arr.__getitem__)\n",
        "\n",
        "def generate_text_from_ngram_model(\n",
        "        start_prompt: str,\n",
        "        n_tokens: int,\n",
        "        ngram_model: dict[str, dict[str, float]],\n",
        "        tokenizer: SimpleArabicCharacterTokenizer,\n",
        "        sampling_mode: Literal[\"random\", \"greedy\"] = \"random\"\n",
        ") -> str:\n",
        "    \"\"\"Generate text based on a starting prompt using an ngram model.\n",
        "\n",
        "    Args:\n",
        "        start_prompt: The initial prompt to start the generation.\n",
        "        n_tokens: The number of tokens to generate after the prompt.\n",
        "        ngram_model: An ngram model mapping contexts of n-1 tokens to distributions\n",
        "            over next token.\n",
        "        tokenizer: The tokenizer to encode and decode text.\n",
        "        sampling_mode: Whether to use random or greedy sampling. Supported\n",
        "            options are \"random\" and \"greedy\".\n",
        "\n",
        "    Returns:\n",
        "        The generated text from the prompt.\n",
        "    \"\"\"\n",
        "    # Infer n-1 from the length of the keys in the model\n",
        "    n_minus_1 = len(next(iter(ngram_model.keys())))\n",
        "\n",
        "    # Tokenize the starting prompt.\n",
        "    generated_tokens = tokenizer.character_tokenize(start_prompt)\n",
        "\n",
        "    for _ in range(n_tokens):\n",
        "        # 1. Determine the context for the n-gram model.\n",
        "        context = \"\".join(generated_tokens[-n_minus_1:])\n",
        "\n",
        "        # 2. Use the n-gram model to get the conditional probabilities.\n",
        "        # If context is not in model, we cannot proceed.\n",
        "        if context not in ngram_model:\n",
        "            break\n",
        "\n",
        "        distribution = ngram_model[context]\n",
        "        possible_next_tokens = list(distribution.keys())\n",
        "        probabilities = list(distribution.values())\n",
        "\n",
        "        # 3. Support \"random\" or \"greedy\" sampling.\n",
        "        if sampling_mode == \"greedy\":\n",
        "            # Greedily pick the token with the highest probability.\n",
        "            next_token_index = argmax(probabilities)\n",
        "            next_token = possible_next_tokens[next_token_index]\n",
        "        elif sampling_mode == \"random\":\n",
        "            # Randomly sample the next token based on its probability.\n",
        "            next_token = random.choices(possible_next_tokens, weights=probabilities, k=1)[0]\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported sampling_mode: {sampling_mode}\")\n",
        "\n",
        "        generated_tokens.append(next_token)\n",
        "\n",
        "    # 4. Return the complete resulting sequence as a string.\n",
        "    generated_text = tokenizer.join_text(generated_tokens)\n",
        "    return generated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H37rhdge_56Q"
      },
      "source": [
        "### Test your text generator\n",
        "\n",
        "You can experiment with your implementation of `generate_text_from_ngram_model` by running this cell.\n",
        "\n",
        "âœ… All the code has been written for you in this part. **You are not required to add or modify code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "neER456I_56R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa882ca7-7c96-43a0-a0b6-c71cf4f96402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start prompt is:\n",
            "\tÙŠÙˆÙ… ÙˆØ§Ø­Ø¯\n",
            "Text generated is:\n",
            "\tâ€®ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯Ø©. Ø³ÙŠÙ„Ø¹Ø¨Ø§Ù† Ù…ÙˆØ¯Ø¹â€¬\n"
          ]
        }
      ],
      "source": [
        "# Train n-gram model from dataset.\n",
        "n = 4 # Size of n-grams.\n",
        "ngram_model = build_ngram_model(dataset, n, tokenizer)\n",
        "\n",
        "# Generate text.\n",
        "start_prompt = \"ÙŠÙˆÙ… ÙˆØ§Ø­Ø¯\"\n",
        "print(f\"Start prompt is:\\n\\t{start_prompt}\")\n",
        "n_tokens = 15 # Specify the number of new tokens to generate.\n",
        "tokenizer = SimpleArabicCharacterTokenizer()\n",
        "\n",
        "generated_text = generate_text_from_ngram_model(\n",
        "    start_prompt,\n",
        "    n_tokens,\n",
        "    ngram_model,\n",
        "    tokenizer,\n",
        "    sampling_mode=\"random\")\n",
        "\n",
        "print(f\"Text generated is:\\n\\t{display_arabic(generated_text)}\")\n",
        "\n",
        "# Do not remove or modify this logging call, it will be used for tracking purposes\n",
        "logger.info(f'Task 3: The total word count for the generated text is: {len(generated_text.split())}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fH5vnsdDZaL2"
      },
      "source": [
        "Go to the **Task 3. Generate Text from an n-gram Model** section of the lab instructions and click **Check my progress** to verify the objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4hoqf-e_56R"
      },
      "source": [
        "## Task 4. Preparing dataset for training character-based language model  \n",
        "\n",
        "Cymbal Chat wants to develop better language models. To `encode` tokens into indices and to `decode` indices back to text tokens, Cymbal Chat extended the `SimpleArabicCharacterTokenizer` that you developed previously. They adapted code from *Lab: Prepare The Dataset For Training an SLM* and *Lab: Train Your Own Small Language Model* from the course *01 Build Your Own Small Language Model*. They called their class `EnhancedTokenizer`.\n",
        "\n",
        "The input to their transformer models will be capped to a maximum number of tokens that the model can process. This number is called the model's **context length**. However, some of the stories that Cymbal Chat want to train on, have more tokens than their model's context length. This will be a problem.\n",
        "\n",
        "For Task 4, you have to write code that will break token sequences up into overlapping segments that will each fit in the context length.\n",
        "This will support different sizes of transformer models including some with a limited maximum number of tokens. Your instructions will appear in the \"ğŸ’» **Your task**\" box after Cymbal Chat's enhanced tokenizer class. First, run Cymbal Chat's code:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2beiZ-N1_56R"
      },
      "source": [
        "### An enhanced character-based tokenizer\n",
        "\n",
        "Cymbal Chat's `EnhancedTokenizer` class includes padding and unknown tokens with the vocabulary from the simple Arabic character tokenizer.\n",
        "\n",
        "Run the cell with Cymbal Chat's tokenizer class. **You are not required to modify the class or any of its methods.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "y_vnu1tp_56R"
      },
      "outputs": [],
      "source": [
        "class EnhancedTokenizer(SimpleArabicCharacterTokenizer):\n",
        "    \"\"\"\n",
        "    Tokenizer provides two key additional functions:\n",
        "    `encode` method to convert the text into a sequence of indices and the\n",
        "    `decode` method to convert indices back into text.\n",
        "\n",
        "    \"\"\"\n",
        "    # Define constants.\n",
        "    UNKNOWN_TOKEN = \"<UNK>\"\n",
        "    PAD_TOKEN = \"<PAD>\"\n",
        "\n",
        "    def __init__(self, corpus: list[str], vocabulary: list[str] | None = None):\n",
        "        \"\"\"Initializes the tokenizer with texts in corpus or with a vocabulary.\n",
        "\n",
        "        Args:\n",
        "          corpus: Input text dataset\n",
        "          vocabulary: A pre-defined vocabulary. If None,\n",
        "              the vocabulary is automatically inferred from the texts.\n",
        "        \"\"\"\n",
        "        super(EnhancedTokenizer).__init__()\n",
        "        if vocabulary is None:\n",
        "            # Build the vocabulary from scratch.\n",
        "            if isinstance(corpus, str):\n",
        "                corpus = [corpus]\n",
        "\n",
        "            # Convert input text sequence to tokens.\n",
        "            tokens = []\n",
        "            for text in corpus:\n",
        "                tokens.extend(self.character_tokenize(text))\n",
        "\n",
        "            # Create a vocabulary comprising of unique tokens.\n",
        "            vocabulary = self.build_vocabulary(tokens)\n",
        "\n",
        "            # Add special unknown and pad tokens to the vocabulary list.\n",
        "            self.vocabulary = (\n",
        "                [self.PAD_TOKEN] + vocabulary + [self.UNKNOWN_TOKEN]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.vocabulary = vocabulary\n",
        "\n",
        "        # Size of vocabulary.\n",
        "        self.vocabulary_size = len(self.vocabulary)\n",
        "\n",
        "        # Create token-to-index and index-to-token mappings.\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        # Loop through all tokens in the vocabulary. enumerate automatically\n",
        "        # assigns a unique index to each token.\n",
        "        for index, token in enumerate(self.vocabulary):\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "\n",
        "        # Map the special tokens to their IDs.\n",
        "        self.pad_token_id = self.token_to_index[self.PAD_TOKEN]\n",
        "        self.unknown_token_id = self.token_to_index[self.UNKNOWN_TOKEN]\n",
        "\n",
        "    def build_vocabulary(self, tokens: list[str]) -> list[str]:\n",
        "        \"\"\"Create a vocabulary list from the list of tokens.\n",
        "\n",
        "        Args:\n",
        "            tokens: The list of tokens in the dataset.\n",
        "\n",
        "        Returns:\n",
        "            List of unique tokens (vocabulary) in the dataset.\n",
        "        \"\"\"\n",
        "        # Build vocabulary of tokens.\n",
        "        vocabulary = sorted(list(set(tokens)))\n",
        "        return vocabulary\n",
        "\n",
        "    def encode(self, text: str) -> list[int]:\n",
        "        \"\"\"Encodes a text sequence into a list of indices.\n",
        "\n",
        "        Args:\n",
        "            text: The input text to be encoded.\n",
        "\n",
        "        Returns:\n",
        "            A list of indices corresponding to the tokens in the input text.\n",
        "        \"\"\"\n",
        "        indices = []\n",
        "\n",
        "        # Convert tokens into indices.\n",
        "        # Replacing out of vocabulary tokens with \"<UNK>\".\n",
        "        indices = []\n",
        "        for token in self.character_tokenize(text):\n",
        "            if token in self.token_to_index:\n",
        "                token_index = self.token_to_index[token]\n",
        "            else:\n",
        "                token_index = self.unknown_token_id\n",
        "            indices.append(token_index)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def decode(self, indices: int | list[int]) -> str:\n",
        "        \"\"\"Decodes a list (or single index) of integers back into tokens.\n",
        "\n",
        "        Args:\n",
        "            indices: A single index or a list of indices to be\n",
        "                decoded into tokens.\n",
        "\n",
        "        Returns:\n",
        "            A string of decoded tokens corresponding to the input indices.\n",
        "        \"\"\"\n",
        "        # Map a sequence of encoded indices back to a string of decoded tokens.\n",
        "        text = \"\"\n",
        "\n",
        "        # Map indices to tokens.\n",
        "        tokens = []\n",
        "        for index in indices:\n",
        "            token = self.index_to_token.get(index, self.unknown_token_id)\n",
        "            tokens.append(token)\n",
        "\n",
        "        # Join the decoded tokens into a single string.\n",
        "        text = self.join_text(tokens)\n",
        "        return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cii0PHfG_56R"
      },
      "source": [
        "### Complete a function that segments an encoded sequence\n",
        "\n",
        "------\n",
        "> ğŸ’» **Your task:**\n",
        ">\n",
        "> Complete the `segment_encoded_sequence` function.\n",
        ">\n",
        "> This function takes an encoded sequence of token ids, `sequence`, as input. It should segment the sequence into subsequences of at most `max_length` length with specified overlap `n_overlap`.\n",
        ">\n",
        "> For example, a text string might be encoded into the sequence of token indices:\n",
        ">\n",
        "> ```\n",
        "> [17, 40, 30, 41, 29, 2, 33, 40, 35, 20]\n",
        "> ```\n",
        ">\n",
        "> If a model's input context length is only three tokens, `max_length=3`, then the longest subsequence is three tokens long. If one token overlaps between subsequences, `n_overlap=1`, then `segment_encoded_sequence` should return the following `subsequences` list of lists:\n",
        "> ```\n",
        "> [ [17, 40, 30],\n",
        ">   [30, 41, 29],\n",
        ">   [29, 2, 33],\n",
        ">   [33, 40, 35],\n",
        ">   [35, 20] ]\n",
        "> ```\n",
        ">\n",
        "> It is clear that the original sequence `[17, 40, 30, 41, 29, 2, 33, 40, 35, 20]` can be reconstructed by simple concatenation of the subsequences with overlaps removed.\n",
        ">\n",
        "> Detailed instructions are marked in the `segment_encoded_sequence` function that you should complete.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "nHwQw1g6_56R"
      },
      "outputs": [],
      "source": [
        "def segment_encoded_sequence(\n",
        "        sequence: list[int],\n",
        "        max_length: int,\n",
        "        n_overlap: int\n",
        ") -> list[list[int]]:\n",
        "    \"\"\"Segment a long encoded sequence into overlapping subsequences of maximum\n",
        "    length.\n",
        "\n",
        "    Divides the input sequence into chunks of max_length tokens with specified\n",
        "    overlap between consecutive segments. The final segment may be shorter than\n",
        "    max_length if insufficient tokens remain.\n",
        "\n",
        "    Args:\n",
        "        sequence: List of token indices to segment.\n",
        "        max_length: Maximum length for each subsequence.\n",
        "        n_overlap: Number of tokens to overlap between consecutive segments.\n",
        "\n",
        "    Returns:\n",
        "        List of subsequences, each with at most max_length token indices. All\n",
        "        segments except possibly the last will have exactly max_length tokens.\n",
        "    \"\"\"\n",
        "    subsequences = []\n",
        "\n",
        "    # Calculate the step size to move the window forward for each new segment.\n",
        "    # This ensures the specified overlap between consecutive segments.\n",
        "    step = max_length - n_overlap\n",
        "\n",
        "    # Iterate through the sequence with the calculated step size.\n",
        "    for i in range(0, len(sequence), step):\n",
        "        # Extract the subsequence of max_length. Python's slicing handles the\n",
        "        # end of the list gracefully, creating a shorter final segment if\n",
        "        # i + max_length exceeds the sequence length.\n",
        "        subsequence = sequence[i:i + max_length]\n",
        "        subsequences.append(subsequence)\n",
        "\n",
        "    return subsequences\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMUCofcU_56S"
      },
      "source": [
        "### Test your segmenter\n",
        "\n",
        "You can experiment with your implementation of `segment_encoded_sequence` by running this cell.\n",
        "\n",
        "âœ… All the code has been written for you in this part. **You are not required to add or modify code.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_Pr3ne47_56S"
      },
      "outputs": [],
      "source": [
        "def segment_encoded_sequence(\n",
        "        sequence: list[int],\n",
        "        max_length: int,\n",
        "        n_overlap: int\n",
        ") -> list[list[int]]:\n",
        "    \"\"\"Segment a long encoded sequence into overlapping subsequences of maximum\n",
        "    length.\n",
        "\n",
        "    Divides the input sequence into chunks of max_length tokens with specified\n",
        "    overlap between consecutive segments. The final segment may be shorter than\n",
        "    max_length if insufficient tokens remain.\n",
        "\n",
        "    Args:\n",
        "        sequence: List of token indices to segment.\n",
        "        max_length: Maximum length for each subsequence.\n",
        "        n_overlap: Number of tokens to overlap between consecutive segments.\n",
        "\n",
        "    Returns:\n",
        "        List of subsequences, each with at most max_length token indices. All\n",
        "        segments except possibly the last will have exactly max_length tokens.\n",
        "    \"\"\"\n",
        "    subsequences = []\n",
        "\n",
        "    # 1. Repeatedly take a segment of (up to) `max_length` from the sequence.\n",
        "    # 2. Ensure that is an overlap of n_overlap between consecutive\n",
        "    #    subsequences.\n",
        "    # 3. Handle the case when the final segment may be shorter than max_length.\n",
        "\n",
        "    # Calculate the step size for moving the window.\n",
        "    step = max_length - n_overlap\n",
        "\n",
        "    # Iterate through the sequence with the calculated step size.\n",
        "    for i in range(0, len(sequence), step):\n",
        "        # Extract the subsequence of max_length.\n",
        "        subsequence = sequence[i:i + max_length]\n",
        "        subsequences.append(subsequence)\n",
        "\n",
        "    return subsequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP_Xmm9DZaL4"
      },
      "source": [
        "Go to the sub-section **Build segment_encoded_sequence** in the section **Task 4. Prepare dataset for training character-based language model**  of the lab instructions and click **Check my progress** to verify the objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93c-sAbq_56S"
      },
      "source": [
        "### Complete a function that creates training inputs and targets\n",
        "\n",
        "Now that you can segment a long sequence of token ids into subsequences of a maximum length, the last task is to take a dataset (of type `list[str]`) and turn it into training inputs and targets:\n",
        "\n",
        "* `inputs`: an array of token sequences, each of length `context_length`.\n",
        "* `targets`: an array of target sequences (inputs shifted by one position), each of length `context_length`.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> ğŸ’» **Your task:**\n",
        ">\n",
        "> Complete the `create_training_sequences` function.\n",
        ">\n",
        "> The function should encode and segment the complete dataset, convert subsequences to padded `numpy` arrays and then prepare `input` and `target` arrays for training a transformer model.\n",
        ">\n",
        "> Your task is to add code to create a list of lists called `encoded_tokens` from an input `dataset`. Each inner list in `encoded_tokens` represents a sequence of scalars (e.g., integers for tokenized text)\n",
        ">\n",
        "> Code to then pad and format `encoded_tokens` into `inputs` and `targets` is provided for you.\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KzfPvrDp_56S"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "# The following are assumed to be defined or imported in your environment\n",
        "# from the previous steps of the lab.\n",
        "# - class EnhancedTokenizer\n",
        "# - def segment_encoded_sequence(...)\n",
        "\n",
        "def create_training_sequences(\n",
        "        dataset: list[str],\n",
        "        context_length: int,\n",
        "        n_overlap: int,\n",
        "        tokenizer: 'EnhancedTokenizer'  # Use quotes for forward reference if needed\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create training input-target sequence pairs from text dataset.\n",
        "\n",
        "    Encodes text data into token sequences, segments them into fixed-length\n",
        "    overlapping windows, and creates input-target pairs for language modeling\n",
        "    where targets are inputs shifted by one position.\n",
        "\n",
        "    Args:\n",
        "        dataset: List of text strings to process into training sequences.\n",
        "        context_length: Maximum sequence length for model input.\n",
        "        n_overlap: Number of tokens to overlap between consecutive segments.\n",
        "        tokenizer: Tokenizer object with an encode method for text-to-tokens\n",
        "            conversion.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (inputs, targets) where:\n",
        "        - inputs: Array of token sequences of length context_length.\n",
        "        - targets: Array of target sequences (inputs shifted by one position).\n",
        "    \"\"\"\n",
        "\n",
        "    segmentation_length = context_length + 1\n",
        "    # The segments are one token longer than the model's maximum input length\n",
        "    # because the target (next) tokens to predict are the input tokens shifted\n",
        "    # by one position.\n",
        "\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    encoded_tokens = []\n",
        "\n",
        "    # 1. Iterate over the entries in the dataset.\n",
        "    for text in dataset:\n",
        "        # 2. For each dataset entry (text), encode it into a sequence of token ids.\n",
        "        sequence_ids = tokenizer.encode(text)\n",
        "\n",
        "        # 3. Segment the sequence of token ids into overlapping segments.\n",
        "        #    This uses the `segment_encoded_sequence` function from the previous task.\n",
        "        segments = segment_encoded_sequence(\n",
        "            sequence_ids, segmentation_length, n_overlap\n",
        "        )\n",
        "\n",
        "        # 4. Include the segments in the list of all encoded tokens.\n",
        "        encoded_tokens.extend(segments)\n",
        "\n",
        "    # The code below was provided and is now correctly fed by `encoded_tokens`.\n",
        "    # It creates padded sequences one token longer than the maximum input length.\n",
        "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "            encoded_tokens,\n",
        "            maxlen=segmentation_length,\n",
        "            padding=\"post\",\n",
        "            value=pad_token_id)\n",
        "\n",
        "    # Create inputs and targets from the padded sequences.\n",
        "    # `inputs` are all tokens except the last one.\n",
        "    inputs = padded_sequences[:, :-1]\n",
        "    # `targets` are all tokens except the first one.\n",
        "    targets = padded_sequences[:, 1:]\n",
        "    return inputs, targets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zS91TZaq_56S"
      },
      "source": [
        "### Test your training sequence creation function\n",
        "\n",
        "You can experiment with your implementation of `create_training_sequence` by running this cell.\n",
        "\n",
        "âœ… All the code has been written for you in this part. **You are not required to add or modify code.**"
      ]
    },
    {
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "def create_training_sequences(\n",
        "        dataset: list[str],\n",
        "        context_length: int,\n",
        "        n_overlap: int,\n",
        "        tokenizer: 'EnhancedTokenizer' # Using quotes for forward reference\n",
        ") -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Create training input-target sequence pairs from text dataset.\n",
        "\n",
        "    Encodes text data into token sequences, segments them into fixed-length\n",
        "    overlapping windows, and creates input-target pairs for language modeling\n",
        "    where targets are inputs shifted by one position.\n",
        "\n",
        "    Args:\n",
        "        dataset: List of text strings to process into training sequences.\n",
        "        context_length: Maximum sequence length for model input.\n",
        "        n_overlap: Number of tokens to overlap between consecutive segments.\n",
        "        tokenizer: Tokenizer object with encode method for text-to-tokens\n",
        "            conversion.\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (inputs, targets) where:\n",
        "        - inputs: Array of token sequences of length context_length.\n",
        "        - targets: Array of target sequences (inputs shifted by one position).\n",
        "    \"\"\"\n",
        "    segmentation_length = context_length + 1\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    encoded_tokens = []\n",
        "\n",
        "    # 1. Iterate over the entries in the dataset.\n",
        "    for text in dataset:\n",
        "        # 2. Encode the text into a sequence of token ids.\n",
        "        sequence_ids = tokenizer.encode(text)\n",
        "\n",
        "        # 3. Segment the sequence of token ids into overlapping segments.\n",
        "        segments = segment_encoded_sequence(\n",
        "            sequence_ids, segmentation_length, n_overlap\n",
        "        )\n",
        "\n",
        "        # 4. Include the segments in the list of encoded tokens.\n",
        "        encoded_tokens.extend(segments)\n",
        "\n",
        "    # The code below is provided in the lab.\n",
        "    # Create padded sequences one token longer than the maximum input length.\n",
        "    padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "            encoded_tokens,\n",
        "            maxlen=segmentation_length,\n",
        "            padding=\"post\",\n",
        "            value=pad_token_id)\n",
        "\n",
        "    # Create inputs and targets from padded sequences.\n",
        "    inputs = padded_sequences[:, :-1]\n",
        "    targets = padded_sequences[:, 1:]\n",
        "    return inputs, targets\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "i1JxOaq9gv-5"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irmgAI3LZaL-"
      },
      "source": [
        "Go to the sub-section **Build create_training_sequences** in the section **Task 4. Prepare dataset for training character-based language model**  of the lab instructions and click **Check my progress** to verify the objective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbcUR9y2VTsa"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This is the end of the Challenge Lab for the **Build your own Small Language Model** course. In this challenge lab, you have:\n",
        "\n",
        "- Developed a character-based tokenizer for Arabic text.\n",
        "- Built a function to generate text from an n-gram model, both by random and greedy sampling.\n",
        "- Implemented functions to segment and prepare character encoded data for the training of transformer-based language models of varying sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_sr-d1qnxUg"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Mohamed, M. and Al-Azani, S. (2025). Enhancing Arabic NLP Tasks through Character-Level Models and Data Augmentation. In Proceedings of the 31st International Conference on Computational Linguistics (pp. 2744-2757).  Retrieved from [https://aclanthology.org/2025.coling-main.186.pdf](https://aclanthology.org/2025.coling-main.186.pdf)\n"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "colab": {
      "collapsed_sections": [
        "2beiZ-N1_56R"
      ],
      "provenance": [],
      "name": "gdm_challenge_lab.ipynb"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}